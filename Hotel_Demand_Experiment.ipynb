{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dataset_local = True\n",
    "if is_dataset_local:\n",
    "    dataset = pd.read_csv('hotel_bookings.csv')\n",
    "else:\n",
    "    dataset = pd.read_csv('https://raw.githubusercontent.com/Sid-darthvader/DoWhy-The-Causal-Story-Behind-Hotel-Booking-Cancellations/master/hotel_bookings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect and clean missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_not_processed = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company variable is almost missing, meaning most of the ID of the company/entity that made the booking or responsible for paying the booking are missing.\n",
    "# We've decided to drop this column as it is not informative to our experiment.\n",
    "\n",
    "dataset = dataset.drop('company', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us observe the agent distribution now as it has 14% nan values\n",
    "\n",
    "sns.histplot(dataset['agent'], kde=True, bins=50, linewidth=3, color=\"black\")\n",
    "\n",
    "plt.xlabel('Agent Fees')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Agent variable has approximately 14% missing values. We will impute these missing values with the most frequent value, as it is significantly predominant.\n",
    "\n",
    "dataset['agent']=dataset['agent'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us observe the distribution after the imuptation and ensure that it preserves the same pattern.\n",
    "\n",
    "sns.histplot(dataset['agent'], kde=True, bins=50, linewidth=3, color=\"black\")\n",
    "\n",
    "plt.xlabel('Agent Fees')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets handle the country distribution\n",
    "\n",
    "country_distribution = dataset['country'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(30, 6))\n",
    "country_distribution.plot(kind='bar', color='skyblue')\n",
    "\n",
    "plt.title('Country Distribution')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=90, ha='right', fontsize=12, color='black', backgroundcolor='white')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the most frequent country is Portugal and by far. We will impute the nan values with PRT.\n",
    "# We only have 0.4% nan values in the country column.\n",
    "\n",
    "dataset['country'] = dataset['country'].replace(np.nan, dataset['country'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have 0.000034 nan values in the \"children\" column, as the percentage is neglectable we are going to delete sush rows.\n",
    "\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not dataset.isnull().values.any(), \"There are stillx NaN or null values in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our preprocessing will simplify certain assumptions. We believe that adults, children, and babies within each family can be grouped together, as they have the same influence in our experiment.\n",
    "# Similarly, the stays_in_week_nights and stays_in_weekend_nights variables can be combined, as they exert the same influence on our analysis and don't have any meaning as their own.\n",
    "\n",
    "dataset['total_guests'] = dataset['adults'] + dataset['babies'] + dataset['children']\n",
    "dataset.drop(['adults', 'babies', 'children'], axis=1, inplace=True)\n",
    "dataset['total_days'] = dataset['stays_in_week_nights'] + dataset['stays_in_weekend_nights']\n",
    "dataset.drop(['stays_in_week_nights', 'stays_in_weekend_nights'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to merge the reserved_room_type and assigned_room_type in one column that is true when the customer get the reserved as the assigned, and false otherwise.\n",
    "\n",
    "dataset[\"different_room_assigned\"]= np.where(dataset[\"reserved_room_type\"] == dataset[\"assigned_room_type\"], 1, 0)\n",
    "dataset.drop(['assigned_room_type', 'reserved_room_type'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping some additional variables that we believe has no strong causal effect in our experiment, and just complicates the causal relations.\n",
    "\n",
    "dataset.drop(['reservation_status_date'], axis=1, inplace=True)\n",
    "dataset.drop(['arrival_date_year'], axis=1, inplace=True)\n",
    "dataset.drop(['arrival_date_week_number'], axis=1, inplace=True)\n",
    "dataset.drop(['arrival_date_day_of_month'], axis=1, inplace=True)\n",
    "dataset.drop(['market_segment'], axis=1, inplace=True)\n",
    "dataset.drop(['customer_type'], axis=1, inplace=True)\n",
    "dataset.drop(['meal'], axis=1, inplace=True)\n",
    "dataset.drop(['reservation_status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the arrival date month from categorical to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "month_mapping = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "\n",
    "dataset['arrival_date_month'] = dataset['arrival_date_month'].map(month_mapping)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "categorical_columns = ['hotel', 'deposit_type', 'distribution_channel', 'country']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    dataset[col] = le.fit_transform(dataset[col])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers:\n",
    "\n",
    "features = dataset.columns.to_list()\n",
    "n = 1\n",
    "plt.figure(figsize=(10,10))\n",
    "for feature in features:\n",
    "    plt.subplot(4, 5, n)\n",
    "    sns.boxplot(dataset[feature])\n",
    "    n += 1\n",
    "    plt.tight_layout()\n",
    "plt.savefig(\"boxplot_outlier_detection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['lead_time'] <= 600]\n",
    "dataset = dataset[dataset['adr'] <= 3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the preprocessed dataset\n",
    "dataset.to_csv(\"dataset_ready.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"different_room_assigned\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets observe first the correlation map between the numerical values.\n",
    "# We can observe a correlation between is_canceled and different_room_assigned, indicating that assigning a different room type may have an impact on booking cancellations.\n",
    "\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "for col in dataset.columns:\n",
    "    if(dataset[col].dtype != 'object'):\n",
    "        numerical_features.append(col)\n",
    "    else:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(categorical_features)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(dataset[numerical_features].corr(), linewidths=2, linecolor='black', annot=True, fmt=\".3f\")\n",
    "plt.savefig(\"correlation_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the scatter plot between each two numerical values\n",
    "\n",
    "pairplot_fig = sns.pairplot(dataset[numerical_features])\n",
    "pairplot_fig.savefig(\"pairplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booking cancellation ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Booking cancellation ratio\n",
    "\n",
    "dataset['is_canceled'].value_counts().plot.pie(autopct='%1.1f%%',shadow=True, colors=['green', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of lead time on cancellation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = ['#FF9999', '#66B2FF']\n",
    "\n",
    "sns.boxplot(x='is_canceled', y='lead_time',data=dataset, palette=custom_palette)\n",
    "plt.savefig(\"LeadTimeEffectOnCancellationRate.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancellation rate for repeated guests vs non repeated guests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bars(ax, data, category_counts):\n",
    "    for p, count in zip(ax.patches, category_counts):\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            percentage = f'{100 * height / count:.1f}%'\n",
    "            x = p.get_x() + p.get_width() / 2\n",
    "            y = height\n",
    "            ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "def display_graph(ax, feat):\n",
    "    category_counts = feat.value_counts().values\n",
    "    annotate_bars(ax, feat, category_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot(x='is_repeated_guest', hue='is_canceled', data=dataset, palette=custom_palette)\n",
    "plt.title(\"Repeated Guest vs Cancellation\", fontweight=\"bold\", size=20)\n",
    "display_graph(ax, dataset.is_repeated_guest)\n",
    "plt.savefig(\"RepeatedGuestVsCancellation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancellation rate for Deposit vs Non-Deposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot(x='deposit_type', hue=\"is_canceled\", data=dataset_not_processed, palette=custom_palette)\n",
    "plt.title(\"Cancellation Count vs Deposit Type\", fontweight=\"bold\", size=20)\n",
    "display_graph(ax, dataset_not_processed.deposit_type)\n",
    "plt.savefig(\"DepositTypeVsCancellation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of the amount of special requests affect the cancellation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot(x='total_of_special_requests', hue=\"is_canceled\", data=dataset, palette=custom_palette)\n",
    "plt.title(\"Cancellation count vs number of special requests\", fontweight=\"bold\", size=20)\n",
    "display_graph(ax, dataset.total_of_special_requests)\n",
    "plt.savefig(\"SpecialRequestsVsCancellation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing, Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the ready dataset\n",
    "dataset=pd.read_csv('dataset_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_variables = dataset.columns.tolist()\n",
    "print(measured_variables)\n",
    "print(f'number of measured variables: {len(measured_variables)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Causal Graph representing our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalgraphicalmodels import CausalGraphicalModel\n",
    "\n",
    "hidden_confounders = ['financial_status_hidden', 'hotel_policies_hidden', 'local_events_hidden']\n",
    "all_variables = measured_variables + hidden_confounders\n",
    "\n",
    "graph = CausalGraphicalModel(\n",
    "    nodes=all_variables,\n",
    "    edges=[\n",
    "        ('distribution_channel', 'lead_time'),\n",
    "        ('financial_status_hidden', 'distribution_channel'),\n",
    "        ('distribution_channel', 'is_canceled'),\n",
    "        ('distribution_channel', 'days_in_waiting_list'),\n",
    "        ('distribution_channel', 'different_room_assigned'),\n",
    "        ('hotel', 'booking_changes'),\n",
    "        ('hotel', 'total_of_special_requests'),\n",
    "        ('hotel', 'local_events_hidden'),\n",
    "        ('hotel', 'total_days'),\n",
    "        ('hotel', 'arrival_date_month'),\n",
    "        ('lead_time', 'is_canceled'), \n",
    "        ('country', 'lead_time'),\n",
    "        ('lead_time', 'days_in_waiting_list'),\n",
    "        ('lead_time', 'deposit_type'),\n",
    "        ('local_events_hidden', 'lead_time'),\n",
    "        ('local_events_hidden', 'booking_changes'),\n",
    "        ('local_events_hidden', 'is_canceled'),\n",
    "        ('local_events_hidden', 'deposit_type'),\n",
    "        ('local_events_hidden', 'total_days'),\n",
    "        ('country', 'local_events_hidden'),\n",
    "        ('country', 'hotel_policies_hidden'),\n",
    "        ('different_room_assigned', 'is_canceled'), \n",
    "        ('total_of_special_requests', 'different_room_assigned'),\n",
    "        ('hotel_policies_hidden', 'different_room_assigned'),\n",
    "        ('total_guests', 'different_room_assigned'),\n",
    "        ('deposit_type', 'different_room_assigned'),\n",
    "        ('is_repeated_guest', 'is_canceled'),\n",
    "        ('days_in_waiting_list', 'is_canceled'),\n",
    "        ('hotel_policies_hidden', 'days_in_waiting_list'),\n",
    "        ('previous_bookings_not_canceled', 'is_canceled'),\n",
    "        ('previous_bookings_not_canceled', 'is_repeated_guest'),\n",
    "        ('financial_status_hidden', 'deposit_type'),\n",
    "        ('financial_status_hidden', 'is_canceled'),\n",
    "        ('arrival_date_month', 'total_days'),\n",
    "        ('is_repeated_guest', 'is_canceled'),\n",
    "        ('total_days', 'is_canceled'),\n",
    "        ('total_days', 'agent'),\n",
    "        ('agent', 'is_canceled'),\n",
    "        ('financial_status_hidden', 'agent'),\n",
    "        ('country', 'agent'),\n",
    "        ('agent', 'days_in_waiting_list'),\n",
    "        ('total_guests', 'is_canceled'),\n",
    "        ('previous_cancellations', 'is_canceled'),\n",
    "        ('previous_cancellations', 'is_repeated_guest'),\n",
    "        ('total_guests', 'required_car_parking_spaces'),\n",
    "        ('total_days', 'required_car_parking_spaces'),\n",
    "        ('total_of_special_requests', 'is_canceled'),\n",
    "        ('booking_changes', 'different_room_assigned'),\n",
    "        ('booking_changes', 'is_canceled'),\n",
    "        ('is_canceled', 'adr'),\n",
    "        ('total_of_special_requests', 'days_in_waiting_list')        \n",
    "    ]\n",
    ")\n",
    "\n",
    "graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure graph is DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Causal Graph is DAG\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, edges):\n",
    "        self.graph = defaultdict(list)\n",
    "        for u, v in edges:\n",
    "            self.graph[u].append(v)\n",
    "    \n",
    "    def find_cycle(self):\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        \n",
    "        nodes = list(self.graph.keys())\n",
    "        \n",
    "        for node in nodes:\n",
    "            if node not in visited:\n",
    "                if self._dfs(node, visited, rec_stack):\n",
    "                    return self.cycle\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _dfs(self, node, visited, rec_stack):\n",
    "        visited.add(node)\n",
    "        rec_stack.add(node)\n",
    "        \n",
    "        for neighbor in self.graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                if self._dfs(neighbor, visited, rec_stack):\n",
    "                    return True\n",
    "            elif neighbor in rec_stack:\n",
    "                # Cycle detected, store the cycle\n",
    "                self.cycle = list(rec_stack)\n",
    "                return True\n",
    "        \n",
    "        rec_stack.remove(node)\n",
    "        return False\n",
    "\n",
    "edges = [\n",
    "    ('distribution_channel', 'lead_time'),\n",
    "    ('financial_status_hidden', 'distribution_channel'),\n",
    "    ('distribution_channel', 'is_canceled'),\n",
    "    ('distribution_channel', 'days_in_waiting_list'),\n",
    "    ('distribution_channel', 'different_room_assigned'),\n",
    "    ('hotel', 'booking_changes'),\n",
    "    ('hotel', 'total_of_special_requests'),\n",
    "    ('hotel', 'local_events_hidden'),\n",
    "    ('hotel', 'total_days'),\n",
    "    ('hotel', 'arrival_date_month'),\n",
    "    ('lead_time', 'is_canceled'), \n",
    "    ('country', 'lead_time'),\n",
    "    ('lead_time', 'days_in_waiting_list'),\n",
    "    ('lead_time', 'deposit_type'),\n",
    "    ('local_events_hidden', 'lead_time'),\n",
    "    ('local_events_hidden', 'booking_changes'),\n",
    "    ('local_events_hidden', 'is_canceled'),\n",
    "    ('local_events_hidden', 'deposit_type'),\n",
    "    ('local_events_hidden', 'total_days'),\n",
    "    ('country', 'local_events_hidden'),\n",
    "    ('country', 'hotel_policies_hidden'),\n",
    "    ('different_room_assigned', 'is_canceled'), \n",
    "    ('total_of_special_requests', 'different_room_assigned'),\n",
    "    ('hotel_policies_hidden', 'different_room_assigned'),\n",
    "    ('total_guests', 'different_room_assigned'),\n",
    "    ('deposit_type', 'different_room_assigned'),\n",
    "    ('is_repeated_guest', 'is_canceled'),\n",
    "    ('days_in_waiting_list', 'is_canceled'),\n",
    "    ('hotel_policies_hidden', 'days_in_waiting_list'),\n",
    "    ('previous_bookings_not_canceled', 'is_canceled'),\n",
    "    ('previous_bookings_not_canceled', 'is_repeated_guest'),\n",
    "    ('financial_status_hidden', 'deposit_type'),\n",
    "    ('financial_status_hidden', 'is_canceled'),\n",
    "    ('arrival_date_month', 'total_days'),\n",
    "    ('is_repeated_guest', 'is_canceled'),\n",
    "    ('total_days', 'is_canceled'),\n",
    "    ('total_days', 'agent'),\n",
    "    ('agent', 'is_canceled'),\n",
    "    ('financial_status_hidden', 'agent'),\n",
    "    ('country', 'agent'),\n",
    "    ('agent', 'days_in_waiting_list'),\n",
    "    ('total_guests', 'is_canceled'),\n",
    "    ('previous_cancellations', 'is_canceled'),\n",
    "    ('previous_cancellations', 'is_repeated_guest'),\n",
    "    ('total_guests', 'required_car_parking_spaces'),\n",
    "    ('total_days', 'required_car_parking_spaces'),\n",
    "    ('total_of_special_requests', 'is_canceled'),\n",
    "    ('booking_changes', 'different_room_assigned'),\n",
    "    ('booking_changes', 'is_canceled'),\n",
    "    ('is_canceled', 'adr'),\n",
    "    ('total_of_special_requests', 'days_in_waiting_list')  \n",
    "]\n",
    "\n",
    "graph = Graph(edges)\n",
    "cycle = graph.find_cycle()\n",
    "\n",
    "if cycle:\n",
    "    print(\"Cycle detected:\", cycle)\n",
    "else:\n",
    "    print(\"No cycle detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import json\n",
    "\n",
    "\n",
    "graph_str = \"\"\"\n",
    "digraph {\n",
    "    \"distribution_channel\" -> \"lead_time\";\n",
    "    \"financial_status_hidden\" -> \"distribution_channel\";\n",
    "    \"distribution_channel\" -> \"is_canceled\";\n",
    "    \"distribution_channel\" -> \"days_in_waiting_list\";\n",
    "    \"distribution_channel\" -> \"different_room_assigned\";\n",
    "    \"hotel\" -> \"booking_changes\";\n",
    "    \"hotel\" -> \"total_of_special_requests\";\n",
    "    \"hotel\" -> \"local_events_hidden\";\n",
    "    \"hotel\" -> \"total_days\";\n",
    "    \"hotel\" -> \"arrival_date_month\";\n",
    "    \"lead_time\" -> \"is_canceled\"; \n",
    "    \"country\" -> \"lead_time\";\n",
    "    \"lead_time\" -> \"days_in_waiting_list\";\n",
    "    \"lead_time\" -> \"deposit_type\";\n",
    "    \"local_events_hidden\" -> \"lead_time\";\n",
    "    \"local_events_hidden\" -> \"booking_changes\";\n",
    "    \"local_events_hidden\" -> \"is_canceled\";\n",
    "    \"local_events_hidden\" -> \"deposit_type\";\n",
    "    \"local_events_hidden\" -> \"total_days\";\n",
    "    \"country\" -> \"local_events_hidden\";\n",
    "    \"country\" -> \"hotel_policies_hidden\";\n",
    "    \"different_room_assigned\" -> \"is_canceled\"; \n",
    "    \"total_of_special_requests\" -> \"different_room_assigned\";\n",
    "    \"hotel_policies_hidden\" -> \"different_room_assigned\";\n",
    "    \"total_guests\" -> \"different_room_assigned\";\n",
    "    \"deposit_type\" -> \"different_room_assigned\";\n",
    "    \"is_repeated_guest\" -> \"is_canceled\";\n",
    "    \"days_in_waiting_list\" -> \"is_canceled\";\n",
    "    \"hotel_policies_hidden\" -> \"days_in_waiting_list\";\n",
    "    \"previous_bookings_not_canceled\" -> \"is_canceled\";\n",
    "    \"previous_bookings_not_canceled\" -> \"is_repeated_guest\";\n",
    "    \"financial_status_hidden\" -> \"deposit_type\";\n",
    "    \"financial_status_hidden\" -> \"is_canceled\";\n",
    "    \"arrival_date_month\" -> \"total_days\";\n",
    "    \"is_repeated_guest\" -> \"is_canceled\";\n",
    "    \"total_days\" -> \"is_canceled\";\n",
    "    \"total_days\" -> \"agent\";\n",
    "    \"agent\" -> \"is_canceled\";\n",
    "    \"financial_status_hidden\" -> \"agent\";\n",
    "    \"country\" -> \"agent\";\n",
    "    \"agent\" -> \"days_in_waiting_list\";\n",
    "    \"total_guests\" -> \"is_canceled\";\n",
    "    \"previous_cancellations\" -> \"is_canceled\";\n",
    "    \"previous_cancellations\" -> \"is_repeated_guest\";\n",
    "    \"total_guests\" -> \"required_car_parking_spaces\";\n",
    "    \"total_days\" -> \"required_car_parking_spaces\";\n",
    "    \"total_of_special_requests\" -> \"is_canceled\";\n",
    "    \"booking_changes\" -> \"different_room_assigned\";\n",
    "    \"booking_changes\" -> \"is_canceled\";\n",
    "    \"is_canceled\" -> \"adr\";\n",
    "    \"total_of_special_requests\" -> \"days_in_waiting_list\";\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "dataset_modeling=dataset.copy()\n",
    "model = CausalModel(\n",
    "    data=dataset_modeling,\n",
    "    treatment='different_room_assigned',\n",
    "    outcome='is_canceled',\n",
    "    graph=graph_str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification\n",
    "identified_estimand = model.identify_effect()\n",
    "print(f\"Identified effects: {identified_estimand}\")\n",
    "print(f\"All identified back-door variables: {json.dumps(identified_estimand.backdoor_variables, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation, Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate ATE and Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing backdoor criterion based on the identification and ate effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing backdoor criterion based on the identification\n",
    "from pathlib import Path\n",
    "\n",
    "identified_estimand.default_backdoor_id = 'backdoor'\n",
    "desired_effect=\"ate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate ATE using the relevant group of variables identified through the backdoor paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATE using Linear model, propensity score weighting, propensity score matching, propensity score stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "estimated_regular_ate = False\n",
    "\n",
    "if not estimated_regular_ate:\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    methods = ['backdoor.linear_regression', 'backdoor.propensity_score_weighting', 'backdoor.propensity_score_matching', 'backdoor.propensity_score_stratification']\n",
    "    methods = ['backdoor.linear_regression'] # DELETE\n",
    "\n",
    "    for method in methods:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=method,\n",
    "                target_units=desired_effect,\n",
    "                confidence_intervals=True,\n",
    "                test_significance=True,\n",
    "            )\n",
    "\n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "            \n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATE/Regular\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{method}_ATE.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATE using {method}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATE using S-Learner, by fitting {LinearRegression, Ridge, Lasso, ElasticNet, DecisionTreeRegressor, RandomForestRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier} to predict the outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import io\n",
    "import sys\n",
    "\n",
    "estimated_Slearner_ate = False \n",
    "\n",
    "if not estimated_Slearner_ate:\n",
    "    warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "    ml_models=[LinearRegression, Ridge, DecisionTreeRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier]\n",
    "\n",
    "    for ml_model in ml_models:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=\"backdoor.econml.metalearners.SLearner\",\n",
    "                method_params={\n",
    "                    \"init_params\": {'overall_model': ml_model()},\n",
    "                    \"fit_params\": {}\n",
    "                },\n",
    "                target_units=desired_effect,\n",
    "                test_significance=True,\n",
    "            )\n",
    "            \n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATE/SLearner\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{ml_model.__name__}_ATE.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATE using {ml_model.__name__}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {ml_model.__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATE using T-Learner, by fitting {LinearRegression, Ridge, Lasso, ElasticNet, DecisionTreeRegressor, RandomForestRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier} to predict the outcome for both models f_0 and f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "\n",
    "estimated_Tlearner_ate = False\n",
    "\n",
    "if not estimated_Tlearner_ate:\n",
    "    warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "    ml_models=[LinearRegression, Ridge, DecisionTreeRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier]\n",
    "\n",
    "    # Estimate the effect using a custom econml model\n",
    "    for ml_model in ml_models:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                method_params={\n",
    "                    \"init_params\": {'models': [ml_model(), ml_model()]},\n",
    "                    \"fit_params\": {}\n",
    "                },\n",
    "                target_units=desired_effect,\n",
    "                test_significance=True,\n",
    "            )\n",
    "\n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATE/TLearner\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{ml_model.__name__}_ATE.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "            \n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATE using {ml_model.__name__}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {ml_model.__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate ATT and Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing backdoor criterion based on the identification and att effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing backdoor criterion based on the identification\n",
    "\n",
    "identified_estimand.default_backdoor_id = 'backdoor'\n",
    "desired_effect=\"att\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate ATT using the relevant group of variables identified through the backdoor paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATT using Linear model, propensity score weighting, propensity score matching, propensity score stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "estimated_regular_att = False\n",
    "\n",
    "if not estimated_regular_att:\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    methods = ['backdoor.linear_regression', 'backdoor.propensity_score_weighting', 'backdoor.propensity_score_matching', 'backdoor.propensity_score_stratification']\n",
    "\n",
    "    for method in methods:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=method,\n",
    "                target_units=desired_effect,\n",
    "                confidence_intervals=True,\n",
    "                test_significance=True,\n",
    "            )\n",
    "\n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATT/Regular\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{method}_ATT.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATT using {method}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATT using S-Learner, by fitting {LinearRegression, Ridge, Lasso, ElasticNet, DecisionTreeRegressor, RandomForestRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier} to predict the outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import io\n",
    "import sys\n",
    "\n",
    "estimated_Slearner_att = False \n",
    "\n",
    "if not estimated_Slearner_att:\n",
    "    warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "    ml_models=[LinearRegression, Ridge,DecisionTreeRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier]\n",
    "    \n",
    "    for ml_model in ml_models:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=\"backdoor.econml.metalearners.SLearner\",\n",
    "                method_params={\n",
    "                    \"init_params\": {'overall_model': ml_model()},\n",
    "                    \"fit_params\": {}\n",
    "                },\n",
    "                target_units=desired_effect,\n",
    "                test_significance=True,\n",
    "            )\n",
    "            \n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATT/SLearner\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{ml_model.__name__}_ATT.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATT using {ml_model.__name__}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {ml_model.__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate ATT using T-Learner, by fitting {LinearRegression, Ridge, Lasso, ElasticNet, DecisionTreeRegressor, RandomForestRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier} to predict the outcome for both models f_0 and f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "\n",
    "estimated_Tlearner_att = False\n",
    "\n",
    "if not estimated_Tlearner_att:\n",
    "    warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "    ml_models=[LinearRegression, Ridge, DecisionTreeRegressor, LogisticRegression, MLPRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, DecisionTreeClassifier]\n",
    "\n",
    "    # Estimate the effect using a custom econml model\n",
    "    for ml_model in ml_models:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                method_params={\n",
    "                    \"init_params\": {'models': [ml_model(), ml_model()]},\n",
    "                    \"fit_params\": {}\n",
    "                },\n",
    "                target_units=desired_effect,\n",
    "                test_significance=True,\n",
    "            )\n",
    "\n",
    "            default_stdout = sys.stdout\n",
    "            sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "            estimate.interpret()\n",
    "\n",
    "            interpretation = buffer.getvalue()\n",
    "            sys.stdout = default_stdout\n",
    "\n",
    "            refute_placebo_treatment = model.refute_estimate(\n",
    "                identified_estimand,\n",
    "                estimate,\n",
    "                method_name=\"placebo_treatment_refuter\",\n",
    "                placebo_type=\"permute\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "            save_path = Path(\"/Users/shadi-omari/Repos/CausalInference/Results/ATT/TLearner\")\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = save_path / f\"{ml_model.__name__}_ATT.txt\"\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(str(estimate))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(interpretation)\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write('refute_placebo_treatment: ' + str(refute_placebo_treatment))\n",
    "                file.write('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "                file.write(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "            \n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Calculating ATT using {ml_model.__name__}: {estimate.value}')\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            estimate.interpret()\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print('refute_placebo_treatment:' + str(refute_placebo_treatment))\n",
    "            print('\\n\\n\\n -----------------------------------------------------------------------\\n')\n",
    "            print(f'Time taken to estimate: {int(minutes)} minutes and {seconds:.2f} seconds\\n')\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error encountered with {ml_model.__name__}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
